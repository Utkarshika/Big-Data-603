# Assignment 1: Big Data topics markdown

## The following are the answers to the questions.

1. *Big Data with example and types.*

   Big Data
 : Big Data is a body of information that is enormous in volume and is always expanding rapidly. No typical data management systems can effectively store or analyze this data because of its magnitude and complexity. Big data is a type of data that is extremely large. One of the example for bigdata is Social media; according to the estimate, Facebook's systems get more than 500 terabytes of fresh data each day. This information is mostly produced by the uploading of images and videos, messaging, leaving comments, etc.  
   (source:https://www.guru99.com/what-is-big-data.html)

   The types of big data are as follows:
   - Batch
   - Streaming
   - Graph
   - Spatio-temporal

2. *6 ‘V’s of Big Data (define each)*

   The 6 V's of Big data are as follows:

     1. **Volumn**:
    Any dataset's size stands out as a distinguishing factor. The magnitude of the data produced and stored in a Big Data system is referred to as volume. Example: Instagram and Twitter.

     2. **Variety**:
    Variety includes the different formats and ways that different sorts of data are arranged and prepared for processing.Big data includes text, picture, audio, video, and sensor data and can be organized, unstructured, or semi-structured.

     3. **Velocity**:
    Whether data is categorized as large data or ordinary data depends on how quickly it accumulates. Systems must be capable of handling the rate and volume of data creation since a lot of this data must be assessed in real-time. Data processing speed indicates that there will always be more data accessible than there was before, but it also suggests that data processing speed must be equally as fast.

     4. **Variability**:
    The data set's inconsistency might make it difficult to handle and maintain. For instance, the consistency of sensor data quality. This is the variability of data.

     5. **Value**:
    Another important factor to think about is value. It's not just how much data we save or process that matters. In order to get insights, it is also necessary to preserve, analyse, and assess data that is valuable and trustworthy.

     6. **Veracity**:
    Veracity is a term used to describe how reliable and high-quality the data is. Big Data's usefulness is still undeniable if the data is not dependable and/or trustworthy. This is especially valid when utilizing constantly updated data. Because of this, the collection and processing of Big Data requires checks and balances at every stage. 

     Source(https://bau.edu/blog/characteristics-of-big-data/)
     Souces(DATA603-Fall2022-Lecture 2-Intro2BigData)


3. *Phases of Big Data analysis (discuss each)*
   
   The Phases of Big Data Analysis consists of the following-

   1. **Data Acquisition and Recording**:

     The procedures for getting data that was produced by a source outside the organization into the organization for use in production are known as data acquisition.
      (Source: https://www.firstsanfranciscopartners.com/blog/defining-data-acquisition-importance/)
       This kind of data is mostly unstructured. It can be magnitudes of times filtered and compressed. We must create effective filters that do not exclude valuable information.Create the appropriate metadata automatically to define the types of data that are being measured and recorded.

   2. **Information Extraction and Cleaning**:

     Filtration includes the elimination of inaccurate or irrelevant data that is not important to the analysis's goal. Here, faulty data refers to records that may be missing or data that contains incompatible data types.Following filtration, a copy of the filtered data is saved and compressed in case it's needed for a subsequent study.
     Now that the data has been filtered, it is possible that some of the entries are incompatible. To address this problem, a distinct step known as the data extraction phase has been devised. The data that don't fit the analysis's fundamental scope are removed and modified at this step.

   3. **Data Intergration, Aggregation and Representation**:

     The data is validated and cleaned in accordance with the enterprise's criteria. However, the data could be dispersed among several databases, and working with several datasets is not recommended. Consequently, the datasets are combined. For instance, two datasets, such as the Student Academic part and the Student Personal Details section, can be connected together using a common variable, such as the student's roll number. Due to the potential for a huge volume of data, this phase necessitates intensive operation. Automation can be taken into account so that these processes are carried out automatically.

   4. **Query Processing, Data Modeling and Analysis**:

     Basic data subsets may be obtained from the whole collection of data using queries.An abstract model known as a data model standardizes how data items relate to one another and organizes data elements.Models for databases, entity relationships, semantic data models, etc.More than simply SQL Querying is required for Big Data Analysis.In the SQL database, many data are not stored.It might be difficult to accomplish some analytical tasks in SQL queries.

   5. **Interpretation**:

     Using the knowledge from the datasets' data interpretation can be done. In order to get value or a specific result from the analysis, some type of representation is necessary. As a result, a variety of tools are utilized to show the data in a graphic format that business users may readily understand.It is claimed that visualization affects how the results are interpreted. Additionally, it gives users the chance to find solutions to hypothetical queries.
     The outcomes can be utilized for process improvement and optimization. It may also be included into the systems as an input to improve performance.

     (Source: https://www.geeksforgeeks.org/big-data-analytics-life-cycle/#:~:text=Data%20Munging(Validation%20and%20Cleaning,Preparation%20for%20Modeling%20and%20Assessment))

4. *Challenges in Big Data analysis (discuss each)*

    The challenges in Big Data analysis are as follows:

     1. **Heterogeneity and Incompleteness**
     - Algorithms for machine analysis anticipate homogenous data and are unable to comprehend subtlety or any other kind of minute deviations.More research is needed to effectively represent, access, and analyze semi-structured data.There may still be some incompleteness and inaccuracies in the data even after data cleaning and error repair.
     2. **Scale**
     - By definition, big data often refers to vast amounts of data stored on many platforms and systems. Creating a consistent and understandable big data architecture for the enormous data sets that businesses are pulling from CRM and ERP systems and other data sources is the first hurdle, according to Szybillo.
     Making little tweaks makes it simpler to focus on insights once you have a feel of the data being gathered, he claimed. Plan an infrastructure that supports gradual adjustments to make it possible. Making significant modifications might result in new issues.
      (Source: https://www.techtarget.com/searchdatamanagement/tip/10-big-data-challenges-and-how-to-address-them)

     3. **Timeliness**
     - Data volume is increasing more quickly than computing capacity.Many expect that data will rise exponentially.Utilizing cloud computing is a viable alternative.Altering the storage subsystem might have an impact on all aspects of data processing (parallel file system vs. solid state drive).

     4. **Privacy**
     - In the context of Big Data, worry over data privacy has increased.Big data may be used to infer new information based on location, time, and other data aspects.Effectively, managing privacy is a technological and a societal issue. Data breach, data brokerage, data discrimination and data storage are some of the privacy concerns in Big data.
     (Source: https://www.talend.com/resources/big-data-privacy/)

     5. **Human Collaboration**
     - Analytics for Big Data should ideally include both algorithmic and human analysis.A big data analysis system has to accommodate input from various human specialists as well as collaborative results exploration.

     (Souces(DATA603-Fall2022-Lecture 2-Intro2BigData))




